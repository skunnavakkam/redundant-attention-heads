{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A current theory of mine is that certain parts of how this model breaks down inputs is through the mlps. I guess we can ablate MLPs to try to figure something out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sudarshanagopalkunnavakkam/Documents/Github/Language Models Updating Priors/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.58s/it]\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from jaxtyping import Float\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gemma-2-2b\", device=\"mps\", dtype=\"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = \"\"\"\n",
    "France, Paris\n",
    "Japan, Tokyo\n",
    "Brazil, Brasília\n",
    "Canada, Ottawa\n",
    "Australia, Canberra\n",
    "Germany, Berlin\n",
    "India, New_Delhi\n",
    "Italy, Rome\n",
    "Russia,\"\"\"\n",
    "\n",
    "correct_answer = \"87\"\n",
    "correct_answer_token = model.to_tokens(correct_answer, prepend_bos=False)\n",
    "\n",
    "tokens = model.to_tokens(A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', 'Moscow', '▁Moskva', '▁Moscou', '▁Mos']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁Moskva', '▁Mos', '▁Moscou', '▁St']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁Moskva', '▁Mos', '▁St', '▁Moscou']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁Moscou', '▁Moskva', '▁Mos', '▁Moskau']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁Moskva', '▁Mos', '▁Moscou', '▁St']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁Moskva', '▁Moscou', '▁Mos', '▁St']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁Moskva', '▁Mos', '▁St', '▁Saint']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁Moskva', '▁Saint', '▁St', '▁Mos']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁Moskva', '▁Saint', '▁St', '▁Mos']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁Moskva', '▁Mos', 'Moscow', '▁St']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁Moskva', '▁Mos', '▁Moscou', '▁St']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁Moskva', '▁Mos', '▁St', '▁Moscou']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁Moskva', 'Moscow', '▁St', '▁Mos']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁Moskva', '▁Mos', '▁St', '▁Moscou']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁St', 'Moscow', '▁Moskva', '▁Saint']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁Moskva', '▁Mos', '▁St', 'Moscow']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁St', '▁Moskva', '▁Mos', '▁Moscou']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁St', '▁Mos', '▁Moskva', '▁Saint']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁Mos', '▁Moskva', '▁St', '▁Moscou']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁Mos', '▁Moskva', 'Moscow', '▁St']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁Moskva', '▁Mos', '▁Moscou', '▁St']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁Moskva', '▁Mos', '▁Moscou', 'Moscow']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁Moskva', '▁St', '▁Moscou', '▁Mos']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁St', '▁Saint', '▁Moskva', '\\n']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', 'Moscow', '▁Moskva', '▁Mos', '▁Kremlin']\n",
      "torch.Size([1, 38, 256000])\n",
      "['▁Moscow', '▁Washington', '▁New', 'Moscow', '▁Mos']\n"
     ]
    }
   ],
   "source": [
    "def mlp_ablation_hook(mlp_out, hook):\n",
    "    mlp_out[:, :, :] = 0\n",
    "\n",
    "\n",
    "# model.blocks[2].hook_mlp_out.add_hook(mlp_ablation_hook)\n",
    "# model(tokens)\n",
    "\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    model.blocks[layer].hook_mlp_out.add_hook(mlp_ablation_hook)\n",
    "    ablated_logits, ablated_cache = model.run_with_cache(tokens)\n",
    "    \n",
    "    print(ablated_logits.shape)\n",
    "    topk = torch.topk(ablated_logits[0, -1, :], 5)\n",
    "    out = model.tokenizer.convert_ids_to_tokens(topk.indices)\n",
    "\n",
    "    model.reset_hooks()\n",
    "    print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
