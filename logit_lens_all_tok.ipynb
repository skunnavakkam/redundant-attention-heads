{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from einops import einsum\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = \"\"\"\n",
    "(28, 59)\n",
    "(86, 175)\n",
    "(13, 29)\n",
    "(37, 77)\n",
    "(55, 113)\n",
    "(84, 171)\n",
    "(66, 135)\n",
    "(85, 173)\n",
    "(27, 57)\n",
    "(15, 33)\n",
    "(94, 191)\n",
    "(37, 77)\n",
    "(14, 31)\n",
    "(42, 87)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_U = model.unembed.W_U\n",
    "\n",
    "tokens_ids = model.to_tokens(A)\n",
    "tokens = model.tokenizer.convert_ids_to_tokens(tokens_ids)\n",
    "space_token_indexes = torch.tensor(\n",
    "    [i for i, token in enumerate(tokens) if token == \"Ä \"] # weird gpt2 stuffs\n",
    ")\n",
    "# these are the tokens before a given prediction\n",
    "\n",
    "logits, activation_cache = model.run_with_cache(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_resids, labels = activation_cache.accumulated_resid(return_labels=True, apply_ln=True) # layer, batch, pos, dim\n",
    "accum_resids = einsum(\"l b p d -> l p d\", accum_resids) # get rid of batch dim\n",
    "accum_resids = einsum(\"l p d, d v -> l p v\", accum_resids, W_U)\n",
    "accum_resids = accum_resids[:, space_token_indexes, :]\n",
    "\n",
    "for layer in accum_resids:\n",
    "    # pos, v\n",
    "    # find top k at each pos\n",
    "    top_k = torch.topk(layer, 1, dim=1)\n",
    "    # reshape to a 1d tensor\n",
    "    top_k = top_k.indices.flatten()\n",
    "    print(model.tokenizer.convert_ids_to_tokens(top_k))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
